{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INFO371 Problem Set: Bayes-Theorem based Spam Filter\n",
        "\n",
        "In this problem set you will use Bayes Theorem to categorize\n",
        "emails from Ling-Spam corpus into spam and non-spam.  Using a single-word-based Bayes approach does not give good results, but this problem set serves as a preparatory\n",
        "work for understanding the Naive Bayes approach.\n",
        "\n",
        "\n",
        "## Ling-Spam emails\n",
        "\n",
        "The corpus contains ~ 2700 emails from academic accounts talking\n",
        "about conferences, deadlines, papers etc, and peppered with wonderful\n",
        "offers of viagra, lottery millions and similar spam messages.  The\n",
        "emails have been converted into a csv file that contains three variables:\n",
        "\n",
        "* spam --> true or false, this email is spam\n",
        "* files --> the original file name for this email (not needed in this HW).\n",
        "* message --> the content of the email in a single line\n",
        "\n",
        "\n",
        "## (5pt) Explore and clean the data\n",
        "\n",
        "First, let's load data and take a closer look at it.\n",
        "\n",
        "1. (2pt) Load the lingspam-emails.csv.bz2 dataset.  Browse a handful of emails, both spam and non-spam ones, to see what kind of text we are working with here.Hint: check out textwrap module to print long strings on multiple lines.\n",
        "  \n",
        "  \n",
        "2. (3pt) Ensure the data is clean: remove all cases with missing spam and empty message field.  We do not care about the file names."
      ],
      "metadata": {
        "id": "J1VCUNc7DJ-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# loading the dataset\n",
        "spamEmail = pd.read_csv(\"/content/lingspam-emails.csv.bz2\", sep=\"\\t\")\n",
        "spamEmail.shape\n",
        "spamEmail.info()\n",
        "spamEmail.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "FaeNe747DM5b",
        "outputId": "4d8ab4bd-5ff8-440b-f115-bbde8c3a50fc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2893 entries, 0 to 2892\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   spam     2893 non-null   bool  \n",
            " 1   files    2893 non-null   object\n",
            " 2   message  2893 non-null   object\n",
            "dtypes: bool(1), object(2)\n",
            "memory usage: 48.2+ KB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    spam          files                                            message\n",
              "0  False    3-1msg1.txt  Subject: re : 2 . 882 s - > np np  > date : su...\n",
              "1  False    3-1msg2.txt  Subject: s - > np + np  the discussion of s - ...\n",
              "2  False    3-1msg3.txt  Subject: 2 . 882 s - > np np  . . . for me it ...\n",
              "3  False  3-375msg1.txt  Subject: gent conference  \" for the listserv \"...\n",
              "4  False  3-378msg1.txt  Subject: query : causatives in korean  could a..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2f07f5c-b42a-453d-912b-aeecff4eae5a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spam</th>\n",
              "      <th>files</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "      <td>3-1msg1.txt</td>\n",
              "      <td>Subject: re : 2 . 882 s - &gt; np np  &gt; date : su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False</td>\n",
              "      <td>3-1msg2.txt</td>\n",
              "      <td>Subject: s - &gt; np + np  the discussion of s - ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "      <td>3-1msg3.txt</td>\n",
              "      <td>Subject: 2 . 882 s - &gt; np np  . . . for me it ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "      <td>3-375msg1.txt</td>\n",
              "      <td>Subject: gent conference  \" for the listserv \"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>False</td>\n",
              "      <td>3-378msg1.txt</td>\n",
              "      <td>Subject: query : causatives in korean  could a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2f07f5c-b42a-453d-912b-aeecff4eae5a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c2f07f5c-b42a-453d-912b-aeecff4eae5a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c2f07f5c-b42a-453d-912b-aeecff4eae5a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d8dbf759-0bae-4d66-9cad-b2ad12149b78\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d8dbf759-0bae-4d66-9cad-b2ad12149b78')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d8dbf759-0bae-4d66-9cad-b2ad12149b78 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "spamEmail",
              "summary": "{\n  \"name\": \"spamEmail\",\n  \"rows\": 2893,\n  \"fields\": [\n    {\n      \"column\": \"spam\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"files\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2893,\n        \"samples\": [\n          \"9-1566msg1.txt\",\n          \"6-189msg1.txt\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"message\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2876,\n        \"samples\": [\n          \"Subject:   poetics - journal of empirical research on literature , media and the arts - edited by : c . j . van rees , the netherlands . \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" contents : volume 22 ( 1994 ) no . 5 h . verdaasdonk analogies as tools for classifying and appraising literary texts . d . s . miall & d . kuiken foregrounding , defamiliarization , and affect : response to literary stories . m . hayward genre recognition of history and fiction . d . f . rossen - knill toward a pragmatics for literary interpretation . \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" \\\" published by elsevier science - north holland for enquiries & free sample copy - freesamples @ elsevier . co . uk\",\n          \"Subject: lexical semantics workshop - - esslli ' 99  esslli-99 workshop on lexical semantics and linking in constraint-based theories august 16-20 , 1999 a workshop held as part of the 11th european summer school in logic , language and information ( esslli-99 ) august 9 - 20 , 1999 , utrecht , the netherlands * * first call for papers * * organiser : valia kordoni ( university of tuebingen ) background : in recent years , there has been an increasing interest among syntacticians in the interface between syntax and word meaning . in constraint-based theories like lfg and hpsg , this interest has led to the development of the lexical mapping theory ( lmt ) and the hierarchical lexicon models , respectively . having as a common starting point their recognition for the importance of word classes for the interface between syntax and lexical semantics , lmt and the hierarchical lexicon models vary both ontologically , and in the range of linguistic phenomena they attempt to explain , some of which include , but in no way are they limited to , the following : - split intransitivity phenomena ( unaccusative vs . unergative verbs ) - variation among verbs of emotion and location - subcategorization alternations and the linking of indirect arguments - morpholexical processes , including causative verbs - complex predicates - symmetric predicates the aim of the workshop is to provide a forum for researchers and advanced ph . d . students to present and discuss approaches on empirical and formal issues related to the syntax - lexical semantics interface in the frameworks of lfg and hpsg . the workshop intends to continue the series of courses and workshops on lexical semantics and on the interactions between morphology , syntax and semantics held at previous summer schools . it is also dedicated to support inter-framework discussions , since it is focussing on the lexical semantics and linking components of both lfg and hpsg . workshop format : the workshop will consist of five sessions with two 30 + 10 - minute presentations in each session . submissions : all researchers in the area , but especially ph . d . students and young researchers , are encouraged to submit a two-page abstract either as hardcopy or electronically ( postscript only ) . submissions should be sent until february 15 , 1999 . notification of acceptance will be given to contributors around april 15 , 1999 . contributors of accepted papers will be asked to provide an extended abstract ( 10 pages ) in latex format to be included in a summer school reader . the deadline for the submission of the extended abstracts is may 31 , 1999 . submissions should be sent to the following address : valia kordoni universitaet tuebingen seminar fuer sprachwissenschaft kleine wilhelmstr . 113 d-72074 tuebingen germany korder @ sfs . nphil . uni-tuebingen . de registration : workshop contributors will be required to register for esslli-99 , but they will be eligible for a reduced registration fee . important dates : feb 15 , 99 : deadline for submissions apr 15 , 99 : notification of acceptance may 31 , 99 : deadline for final copy aug 16 , 99 : start of workshop further information : the workshop will take place in association with the 11th european summer school in logic , language and information ( esslli ) to be held in utrecht , the netherlands ( 9-20 august 1999 ) . the main focus of the european summer schools in logic , language and information is the interface between linguistics , logic , and computation . the esslli summer school is organized under the auspices of the european association for logic , language and information ( folli ) . foundational , introductory and advanced courses together with workshops cover a wide variety of topics within six areas of interest : logic , computation , language , logic and computation , computation and language , language and logic . previous summer schools have been highly successful , attracting around 500 students from europe and elsewhere . the school has developed into an important meeting place and forum for discussion for students and researchers interested in the interdisciplinary study of logic , language and information . to obtain further information about esslli-99 please visit the esslli-99 home page at http : / / esslli . let . uu . nl /\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import wrap\n",
        "afew = spamEmail.sample(6)\n",
        "for i in range(len(afew)):\n",
        "    print(\"\\n---\\n\", afew.spam.iloc[i])\n",
        "    print(\"\\n\".join(wrap(afew.message.iloc[i])), \"\\n\")"
      ],
      "metadata": {
        "id": "6E1RY171HSTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed50378-28df-441a-c4c5-c93e6b2d006f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            " False\n",
            "Subject: aaai fall symoposium on formalizing context  formalizing\n",
            "context aaai-95 fall symposium mit , cambridge , massachusetts\n",
            "november 10-12 , 1995 call for papers description the notion of\n",
            "context has played an important role in ai systems for many years .\n",
            "however , formal logical explication of contexts remains an area of\n",
            "research in which there are significant open issues . this symposium\n",
            "will provide a forum for discussing formalizations of contexts ,\n",
            "approaches to resolving open issues , and application areas for\n",
            "context formalisms . the most ambitious goal of formalizing contexts\n",
            "is to make automated reasoning systems which are never permanently\n",
            "stuck with the concepts they use at a given time because they can\n",
            "always transcend the context they are in . such a capability would\n",
            "allow the designer of a reasoning system to include only such\n",
            "phenomena as are required for the system 's immediate purpose ,\n",
            "retaining the assurance that if a broader system is required later , \"\n",
            "lifting rules \" can be devised to restate the facts from the narrow\n",
            "context in the broader context with qualifications added as necessary\n",
            ". a formal theory of context in which sentences are always considered\n",
            "as asserted within a context could provide a basis for such\n",
            "transcendence . formal theories of context are also needed to provide\n",
            "a representation of the context associated with a particular\n",
            "circumstance , e . g . the context of a conversation in which terms\n",
            "have particular meanings that they would n't have in the language in\n",
            "general . linguists and philosophers have already studied similar\n",
            "notions of context . an example is the situation theory that has been\n",
            "proposed in philosophy and applied to linguistics . however , these\n",
            "theories usually lie embedded in the analysis of specific linguistic\n",
            "constructions , so locating the exact match with ai concerns is itself\n",
            "a research challenge . this symposium aims to bring together\n",
            "researchers who have studied or applied contexts in ai or related\n",
            "fields . technical papers dealing with formalizations of context , the\n",
            "problem of generality , and use of context in common sense reasoning\n",
            "are especially welcome . however , survey papers which focus on\n",
            "contexts from other points of view , such as philosophy , linguistics\n",
            ", or natural language processing , or which apply contexts in other\n",
            "areas of ai , are also encouraged . submission information persons\n",
            "wishing to make presentations at the workshop should submit papers (\n",
            "up to 12 pages , 12pt font ) . persons wishing only to attend the\n",
            "workshop should submit a 1 - 2 page research summary including a list\n",
            "of relevant publications . a postscript file or 8 paper copies should\n",
            "be sent to the program chair . limited funding will be available to\n",
            "support student travel . timetable april 14 , 1995 submission deadline\n",
            ". may 19 , 1995 submitters will be informed of decisions august 15 ,\n",
            "1995 final papers due september 20 , 1995 aaai will mail working notes\n",
            "to the participants november 10-12 , 1995 fall symposium program\n",
            "committee sasa buvac ( chair ) . department of computer science ,\n",
            "stanford university , stanford ca 94305-2140 . buvac @ sail . stanford\n",
            ". edu richard fikes . knowledge systems laboratory , stanford\n",
            "university , 701 welch road , bldg . c , palo alto , ca 94304 . fikes\n",
            "@ ksl . stanford . edu ramanathan guha . mcc , 3500 , w . balcones\n",
            "center drive , austin , tx 78759 . guha @ mcc . com pat hayes .\n",
            "beckman institute , 405 north mathews av . , urbana , il 61801 .\n",
            "phayes @ cs . uiuc . edu john mccarthy . department of computer\n",
            "science , stanford university , stanford ca 94305-2140 . jmc @ sail .\n",
            "stanford . edu murray shanahan . imperial college , dept of computing\n",
            ", 180 queen 's gate , london sw7 2bz , england . mps @ doc . ic . ac .\n",
            "uk robert stalnaker . 20d-220 , department of linguistics and\n",
            "philosophy , m . i . t . cambridge , ma 02139 . stal @ mit . edu johan\n",
            "van benthem . institute for logic , language and computation ,\n",
            "university of amsterdam , plantage muidergracht 24 , 1018 tv amsterdam\n",
            ", the netherlands . johan @ fwi . uva . nl ( postscript and plain text\n",
            "versions of this announcement are available through the symposium www\n",
            "page at http : / / sail . stanford . edu / buvac / 95 - context-\n",
            "symposium and via anonymous ftp from sail . stanford . edu in the\n",
            "directory / buvac / 95 - context-symposium . ) \n",
            "\n",
            "\n",
            "---\n",
            " False\n",
            "Subject: toc : anthropological linguistics , vol . 40 , no . 2  * *\n",
            "anthropological linguistics , volume 40 , number 2 ( summer 1998 ) * *\n",
            "contents symposium on irrealis irrealis in pilaga and toba ? syntactic\n",
            "versus pragmatic coding , alejandra vidal and harriet e . manelis\n",
            "klein irrealis constructions in mocho ( mayan ) , laura martin\n",
            "irrealis and perfect in itzaj maya , charles andrew hofling lake miwok\n",
            "irrealis , catherine a . callaghan is irrealis a grammatical category\n",
            "in upper chehalis ? , m . dale kinkade irrealis as category , meaning\n",
            ", or reference , edward h . bendix \" irrealis \" as a grammatical\n",
            "category , joan l . bybee _ _ _ _ _ _ _ _ _ _ _ _ _ automatic\n",
            "componential analysis of kinship semantics with a proposed structural\n",
            "solution to the problem of multiple models , vladimir pericliev and\n",
            "raul e . valdes-perez review essay semitic and indo - european : the\n",
            "principal etymologies , with observations on afro - asiatic ( saul\n",
            "levin ) , carleton t . hodge discussion and debate rejoinder , j .\n",
            "marshall unger book reviews the tongue is fire : south african\n",
            "storytellers and apartheid ( harold scheub ) , robert k . herbert\n",
            "korle meets the sea : a sociolinguistic history of accra ( m . e .\n",
            "kropp dakubu ) , adams bodomo a language of our own : the genesis of\n",
            "michif , the mixed cree - french language of the canadian metis (\n",
            "peter bakker ) , patrick douaud contactos y transferencias\n",
            "linguisticas en hispanoamerica ( signo y sena : revista del instituto\n",
            "de linguistica 6 ) , yolanda lastra language contact in japan : a\n",
            "socio - linguistic history ( leo j . loveday ) , j . marshall unger\n",
            "aryans and british india ( thomas r . trautmann ) , garland cannon\n",
            "negotiating identity : rhetoric , metaphor , and social drama in\n",
            "northern ireland ( anthony d . buckley and mary catherine kenney ) ,\n",
            "steve coleman conceptual structure , discourse , and language ( adele\n",
            "e . goldberg ) , william a . foley speech acts and conversational\n",
            "interaction : toward a theory of conversational competence ( michael l\n",
            ". geis ) , jef verschueren historical syntax in cross - linguistic\n",
            "perspective ( alice c . harris and lyle campbell ) , h . paul manning\n",
            "* * * * * * * annual subscription rates ( for 4 issues ) : $ 30 for u\n",
            ". s . individuals ; $ 38 for non - u . s . individuals ; $ 65 for u .\n",
            "s . institutions ; $ 75 for non - u . s . institutions . payment\n",
            "should be in u . s . funds by check or postal money order made payable\n",
            "to anthropological linguistics . visa and mastercard are also accepted\n",
            ". subscriptions and inquires should be sent to : anthropological\n",
            "linguistics , student building 130 ( c ) , indiana university ,\n",
            "bloomington , in 47405 usa ; fax : ( 812 ) 855-7529 ; e-mail : <\n",
            "anthling @ indiana . edu > . for abstracts and more information ,\n",
            "visit our website at : < http : / / www . indiana . edu / ~ anthling > \n",
            "\n",
            "\n",
            "---\n",
            " False\n",
            "Subject: re : 8 . 826 , disc : evolution analytic > synthetic  just to\n",
            "add a little more to the value judgment part of martin haspelmath 's\n",
            "very clear explication of current views of the evolution of typology ,\n",
            "i should point out that otto jespersen believed that the evolution\n",
            "from synthetic to analytic ( such as has happened between old and\n",
            "modern english ) was an overall improvement , with an assumption that\n",
            "totally isolating languages like chinese represented the ideal goal of\n",
            "languages . i do n't have my copy easily available , but i believe\n",
            "this view can be found in the philosophy of grammar . i have heard it\n",
            "suggested that the reason j believed this was he believed english was\n",
            "close to an ideal language . i second martin 's claim that the view\n",
            "that there is a fairly clear consensus among historical linguists\n",
            "about the directionality he discusses . current introductory texts\n",
            "certainly include discussion of this view - - a nice discussion can be\n",
            "found , for example in terry crowley 's _ an introduction to\n",
            "historical linguistics _ ( oxford , 1992 ) , and similar discussions\n",
            "can be found in other current texts . geoff geoffrey s . nathan\n",
            "department of linguistics southern illinois university at carbondale ,\n",
            "carbondale , il , 62901 usa phone : + 618 453-3421 ( office ) fax +\n",
            "618 453-6527 + 618 549-0106 ( home ) \n",
            "\n",
            "\n",
            "---\n",
            " False\n",
            "Subject: job announcement for a spanish sociolinguist at ohio state\n",
            "univ .  department of spanish and portuguese the ohio state university\n",
            "position notice assistant professor of spanish with specialization in\n",
            "sociolinguistics / spanish in the u . s . a . assistant professor of\n",
            "spanish with specialization in sociolinguistics / spanish in the u . s\n",
            ". a . , and demonstrated research focus in language variation and\n",
            "change . solid background in one or more core areas of theoretical\n",
            "linguistics : phonology , morphology , syntax , semantics . teaching\n",
            "in undergraduate language and linguistics courses , and in the\n",
            "linguistics graduate program . tenure - track position , to begin\n",
            "autumn 1996 . ph . d . by time of appointment . teaching experience\n",
            "preferred ; publications desirable . native or near-native spanish\n",
            "language skills . salary commensurate with experience . send\n",
            "application materials ( letter , cv , three recent letters of\n",
            "reference , and one sample publication or dissertation chapter ) by\n",
            "october 31 , 1995 to : stephen summerhill , chair dept . of spanish\n",
            "and portuguese the ohio state university 266 cunz hall 1841 millikin\n",
            "road columbus , oh . 43210-1229 \n",
            "\n",
            "\n",
            "---\n",
            " False\n",
            "Subject: ecology of language acquisition  ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
            "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
            "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ecology of language\n",
            "acquisition international research workshop university of amsterdam ,\n",
            "netherlands 11 - 15 january 1999 * * * * * * * * first announcement *\n",
            "* * * * * * * this workshop re-examines certain assumptions implicit\n",
            "in much language acquisition research to date , such as the primacy of\n",
            "the one-speaker - one-hearer interaction ; the predominance of verbal\n",
            "over prosodic and paralinguistic dimensions of talk ; a static\n",
            "interpretation of \" context \" and participant roles ; and the\n",
            "presupposition of a monolingual / monocultural social matrix . the\n",
            "focus of the meeting is thus the complexity of circumstances in which\n",
            "the language acquirer operates , addressed in such questions as : *\n",
            "how should discourse-analytic and anthropological descriptions of\n",
            "communicative interaction be integrated to account not only for \"\n",
            "conversation \" , multi-party and ritual talk , but also man-machine\n",
            "interaction and forms of virtual participation in the networks of\n",
            "cyberspace ? * how should the notion of \" shared context \" be extended\n",
            "to capture the floor shifts and on-line construction of meaning that\n",
            "take place over the progress of an unfolding discourse ? * how can\n",
            "theories of acquisition be made more sensitive to complex linguistic\n",
            "and sociocultural environments that are to varying degrees plural ,\n",
            "mixed , and in flux ? the intention is to bring together people and\n",
            "paradigms from l1 and l2 acquisition research with the aim of\n",
            "exploring from an empirical base how the multiple contexts of language\n",
            "acquisition are interrelated , and how , with ecosystemic validity ,\n",
            "such interrelations may be theoretically modelled . participation\n",
            "although we are obliged to limit \" real \" participation to 30 people ,\n",
            "we are making provision for a form of remote partial participation via\n",
            "e-mail ( with possible audio links ) . further details will follow in\n",
            "future announcements . meanwhile , if you think you will be interested\n",
            "in participating in either capacity , it would help our planning to\n",
            "send us an e-mail message to this effect . in the programme there will\n",
            "be room for some 12 papers , thematically grouped , with keynote\n",
            "speakers . if you would like to propose a paper , please send in an\n",
            "abstract ( maximum 300 words ) by e-mail before 15 april to : e-mail :\n",
            "ecolang @ hum . uva . nl ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
            "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
            "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ web page : http : / / www . let .\n",
            "uva . nl / ~ ecolang ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
            "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
            "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ this workshop is being organized by jet\n",
            "van dam , jonathan leather and anne bannink ( faculty of humanities ,\n",
            "university of amsterdam ) \n",
            "\n",
            "\n",
            "---\n",
            " False\n",
            "Subject: available for review  the books listed below are in the\n",
            "linguist office and now available for review . if you are interested\n",
            "in reviewing a book ( or leading a discussion of the book ) ; please\n",
            "contact our book review editor , andrew carnie , at : carnie @\n",
            "linguistlist . org please include in your request message a brief\n",
            "statement about your research interests , background , affiliation and\n",
            "other information that might be valuable to help us select a suitable\n",
            "reviewer . turkish grammar : kornfilt , jaklin ( 1997 ) turkish ,\n",
            "descriptive grammars series . bernard comrie ( ed ) . routledge :\n",
            "london . based on comrie 's grammar questionnaire ( 77 ) , this is a\n",
            "comprehensive grammar of turkish with emphasis on syntax , morphology\n",
            "and phonology . an erratum sheet for this grammar is available\n",
            "directly from the author at kornfilt @ mailbox . syr . edu\n",
            "sociolinguistics / history of linguistics : paulston , christina & g .\n",
            "richard tucker ( eds ) ( 1997 ) the early days of sociolinguistics :\n",
            "meomires and reflections . summer institute of linguistics . austin .\n",
            "a compendium of 36 aritcles by participants in teh development of the\n",
            "field of sociolinguistics . the volume provides an insider 's\n",
            "perspective on the , issues both practical and theoretical which\n",
            "motivated individuals and institutions to turn to a view of langauge\n",
            "as inextricably connected to society and culture . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spamEmail = spamEmail.drop(columns=['files'])\n",
        "spamEmail = spamEmail.dropna()"
      ],
      "metadata": {
        "id": "AheiDt18LyRg"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (15pt) Create Document-term matrix (DTM)\n",
        "\n",
        "The first serious step is to create the document-term matrix (DTM).\n",
        "This is simply numeric indicators for selected words: does this email\n",
        "contain the word (1) or not (0).  But before we get there, we have to\n",
        "decide the words.\n",
        "\n",
        "\n",
        "1. (2pt) Choose 10+ words which might be good to distinguish between spam/non-spam.  Use these four: ''viagra'', ''deadline'', ''million'', and ''and''.  Choose more words yourself (you may want to return here and reconsider your choice later).\n",
        "\n",
        "\n",
        "2. (10pt) Convert your messages into DTM.  We do not use the full 60k-words DTM here but only a baby-DTM of the 10 words you picked above. You may add the DTM columns to the original data frame, or keep those in a separate structure.\n",
        "\n",
        "Creating the DTM involves finding whether the word is contained in the message for all emails in data. You can loop over emails and check each one individually, but pandas string methods make life much easier.  You will want to do case-insensitive matching, checking for both upper and lower case.  You may consider something like this:\n",
        "\n",
        "```\n",
        "for w in list_of_words:\n",
        "    emails[w] = emails.message.str.lower().str.contains(w)\n",
        "```\n",
        "\n",
        "  Note: It is more intuitive to work with your data if you\n",
        "  convert the logical values returned by contains to numbers.\n",
        "  \n",
        "  \n",
        "  \n",
        "3. (3pt) Split your work data (i.e. the DTM) and target (the spam indicator) into training and validation chunks (80/20 is a good split)."
      ],
      "metadata": {
        "id": "t3cUk7TVDNXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['viagra', 'deadline', 'million', 'and', 'unsubscribe', 'urgent', 'cialis', 'loan', 'suspended', 'opt-out']\n",
        "\n",
        "for w in words:\n",
        "    spamEmail[w] = spamEmail.message.str.lower().str.contains(w).astype(int)\n",
        "\n",
        "\n",
        "print(spamEmail.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8NDSnuTDQcv",
        "outputId": "90dd8360-508b-49a9-8569-76220590861c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    spam                                            message  viagra  deadline  \\\n",
            "0  False  Subject: re : 2 . 882 s - > np np  > date : su...       0         0   \n",
            "1  False  Subject: s - > np + np  the discussion of s - ...       0         0   \n",
            "2  False  Subject: 2 . 882 s - > np np  . . . for me it ...       0         0   \n",
            "3  False  Subject: gent conference  \" for the listserv \"...       0         0   \n",
            "4  False  Subject: query : causatives in korean  could a...       0         0   \n",
            "\n",
            "   million  and  unsubscribe  urgent  cialis  loan  suspended  opt-out  \n",
            "0        0    1            0       0       0     0          0        0  \n",
            "1        0    0            0       0       0     0          0        0  \n",
            "2        0    0            0       0       0     0          0        0  \n",
            "3        0    1            0       0       0     0          0        0  \n",
            "4        0    1            0       0       0     0          0        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = spamEmail[['viagra', 'deadline', 'million', 'and', 'unsubscribe', 'urgent', 'cialis', 'loan', 'suspended', 'opt-out']]\n",
        "y = spamEmail['spam'].astype(int)\n",
        "\n",
        "Xt, Xv, yt, yv = train_test_split(X, y, test_size=0.20)\n",
        "Xt.shape, Xv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF8BHRtiwO2T",
        "outputId": "c4269d57-e7be-4359-dcea-258bda0d3ef1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2314, 10), (579, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (80pt) Estimate and validate\n",
        "\n",
        "Now you are ready with the preparatory work and it's time to\n",
        "dive into the real thing.  Let's rehearse the Bayes theorem here\n",
        "again.  We want to estimate the probability that an email is spam, given it\n",
        "contains a certain word:\n",
        "\n",
        "$Pr(category = S|w = 1) = \\frac{Pr(w=1|category = S) * Pr(category=S)}{Pr(w=1)}$.\n",
        "\n",
        "\n",
        "In order to compute this probability, we need to calculate some other\n",
        "probabilities:\n",
        "\n",
        "* $Pr(category=S)$ --> Probability of spam in data\n",
        "\n",
        "* $Pr(category=NS)$ --> Probablility for non-spam in data\n",
        "\n",
        "* $Pr(w=1)$ --> Probability the word is seen in messages\n",
        "\n",
        "* $Pr(w=0)$ --> probability the word is not seen in messages\n",
        "\n",
        "* $Pr(w=1|category = S)$ --> & probability the word is seen in messages that are spam\n",
        "\n",
        "* $Pr(w=1|category = NS)$ --> probability the word is seen in messages that are not spam\n",
        "\n",
        "....\n",
        "\n",
        "\n",
        "but it turns out we are still not done with preparations. Namely, you need to compute\n",
        "quite a few different probabilities below, including $Pr(category=S)$, $Pr(category=NS)$, $Pr(w=1)$, $Pr(w=0)$, $Pr(w=1|category = S)$, $Pr(w=0|category = S)$, $Pr(w=1|category = NS)$, $Pr(w=0|category = NS)$.\n",
        "\n",
        "\n",
        "1. (2pt) Design a scheme for your variable names that describes these probabilities so that a) you understand what they mean; and b) the others (including your grader) will understand those! Hint: you may get some ideas from the [Python notes](https://faculty.washington.edu/otoomet/machinelearning-py/python.html#base-language) in Section 2.3, Base Language.\n",
        "\n",
        "The first task is to compute these probabilities.\n",
        "Use only training data for this task.\n"
      ],
      "metadata": {
        "id": "Ex5PRbofx55Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SE # probability of spam email out of all emails\n",
        "NSE # probability of non-spam emails out of all emails\n",
        "\n",
        "P_YW # probability of yes word in all emails\n",
        "P_NW # probability of no word in all emails\n",
        "\n",
        "P_YW_YS # probability of word given it is a spam email\n",
        "P_NW_YS # probability of no word given it is a spam email\n",
        "P_YW_NS # probability of word given it is not a spam email\n",
        "P_NW_NS # probability of no word given it is not a spam email\n"
      ],
      "metadata": {
        "id": "ZOdlj9aJyDOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. (4pt) Compute the priors, the unconditional probabilities for an email being spam and non-spam, $Pr(category=S)$ and $Pr(category=NS)$.  These probabilities are based on the spam variable alone, not on the text."
      ],
      "metadata": {
        "id": "VQzCAzWSLA31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = pd.concat([Xt, yt], axis=1)\n",
        "\n",
        "SE = training_set['spam'].mean()\n",
        "NSE = 1 - SE\n",
        "\n",
        "print(SE)\n",
        "print(NSE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKfO_6nYLJfd",
        "outputId": "2d545c8f-93b7-4865-a0b8-a36f9b5ce545"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1598962834917891\n",
            "0.8401037165082109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next tasks involve computing the following probabilities for each\n",
        "word out of the list of 10 you picked above,\n",
        "I recommend to avoid unneccessary complexity and\n",
        "just to write a loop over the words, compute the\n",
        "answers, and print the word and the corresponding results there.  \n",
        "\n",
        "\n",
        "\n",
        "3. (4pt) For each word $w$, compute the normalizers, $Pr(w=1)$ and $Pr(w=0)$.\n",
        "  \n",
        "  Hint: this is $Pr(million = 1) = 0.0484$.  But note this value\n",
        "  (and the following hints) depends on your random training/validation split!\n",
        "  "
      ],
      "metadata": {
        "id": "aoXPAQS_XAFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in words:\n",
        "    P_YW = np.mean(training_set[i])\n",
        "    P_NW = 1 - P_YW\n",
        "    print(i + \": with word: \" + str(P_YW) + \" no word: \" + str(P_NW))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAO7zrt5XdAR",
        "outputId": "0370f36b-209a-49ca-abc5-377cf196fb5f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "viagra: with word: 0.000432152117545376 no word: 0.9995678478824547\n",
            "deadline: with word: 0.1512532411408816 no word: 0.8487467588591184\n",
            "million: with word: 0.04753673292999136 no word: 0.9524632670700086\n",
            "and: with word: 0.9446845289541919 no word: 0.05531547104580814\n",
            "unsubscribe: with word: 0.008210890233362144 no word: 0.9917891097666378\n",
            "urgent: with word: 0.005185825410544511 no word: 0.9948141745894555\n",
            "cialis: with word: 0.02895419187554019 no word: 0.9710458081244598\n",
            "loan: with word: 0.016853932584269662 no word: 0.9831460674157303\n",
            "suspended: with word: 0.00216076058772688 no word: 0.9978392394122731\n",
            "opt-out: with word: 0.001728608470181504 no word: 0.9982713915298185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. (7pt) For each word $w$, compute $Pr(w=1|category = S)$ and $Pr(w=1|category = NS)$.  These probabilities are based on both the spam-variable and on the DTM component that corresponds to the word $w$.\n",
        "  \n",
        "  Hint: $Pr(million = 1|category = S) = 0.252$"
      ],
      "metadata": {
        "id": "zuiN11fBg-cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in words:\n",
        "    P_YW_YS = training_set[(training_set[i]==1) & (training_set['spam']==1)].shape[0] / training_set['spam'].sum()\n",
        "    P_NW_YS = 1 - P_YW_YS\n",
        "    P_YW_NS =  training_set[(training_set[i]==1) & (training_set['spam']==0)].shape[0] / ( training_set.shape[0] - training_set[i].sum())\n",
        "    P_NW_NS = 1 - P_YW_NS\n",
        "\n",
        "    print(i)\n",
        "    print(\"P_YW_YS: \"+ str(P_YW_YS))\n",
        "    print(\"P_NW_YS: \"+str(P_NW_YS))\n",
        "    print(\"P_YW_NS: \"+str(P_YW_NS))\n",
        "    print(\"P_NW_NS: \"+str(P_NW_NS))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BipzPjVxhHkA",
        "outputId": "f9854f8b-b583-4750-f049-7e30572f132b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "viagra\n",
            "P_YW_YS: 0.002702702702702703\n",
            "P_NW_YS: 0.9972972972972973\n",
            "P_YW_NS: 0.0\n",
            "P_NW_NS: 1.0\n",
            "deadline\n",
            "P_YW_YS: 0.0\n",
            "P_NW_YS: 1.0\n",
            "P_YW_NS: 0.17820773930753564\n",
            "P_NW_NS: 0.8217922606924644\n",
            "million\n",
            "P_YW_YS: 0.24594594594594596\n",
            "P_NW_YS: 0.754054054054054\n",
            "P_YW_NS: 0.008620689655172414\n",
            "P_NW_NS: 0.9913793103448276\n",
            "and\n",
            "P_YW_YS: 0.9216216216216216\n",
            "P_NW_YS: 0.07837837837837835\n",
            "P_YW_NS: 14.4140625\n",
            "P_NW_NS: -13.4140625\n",
            "unsubscribe\n",
            "P_YW_YS: 0.04594594594594595\n",
            "P_NW_YS: 0.9540540540540541\n",
            "P_YW_NS: 0.0008714596949891067\n",
            "P_NW_NS: 0.9991285403050109\n",
            "urgent\n",
            "P_YW_YS: 0.008108108108108109\n",
            "P_NW_YS: 0.9918918918918919\n",
            "P_YW_NS: 0.003909643788010426\n",
            "P_NW_NS: 0.9960903562119896\n",
            "cialis\n",
            "P_YW_YS: 0.010810810810810811\n",
            "P_NW_YS: 0.9891891891891892\n",
            "P_YW_NS: 0.028037383177570093\n",
            "P_NW_NS: 0.9719626168224299\n",
            "loan\n",
            "P_YW_YS: 0.04864864864864865\n",
            "P_NW_YS: 0.9513513513513514\n",
            "P_YW_NS: 0.009230769230769232\n",
            "P_NW_NS: 0.9907692307692307\n",
            "suspended\n",
            "P_YW_YS: 0.010810810810810811\n",
            "P_NW_YS: 0.9891891891891892\n",
            "P_YW_NS: 0.00043308791684711995\n",
            "P_NW_NS: 0.9995669120831528\n",
            "opt-out\n",
            "P_YW_YS: 0.010810810810810811\n",
            "P_NW_YS: 0.9891891891891892\n",
            "P_YW_NS: 0.0\n",
            "P_NW_NS: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. (5pt) Finally, compute the probabilities of interest, $Pr(category = S|w = 1)$ and $Pr(category = S|w = 0)$.  Compute this value using Bayes theorem, not directly by counting!\n",
        "  \n",
        "  For the check, you may also compute\n",
        "  $Pr(category = NS|w = 1)$ and $Pr(category = NS|w = 0)$\n",
        "  \n",
        "  Hint: $\\Pr(\\mathit{category} = S|\\mathit{million} = 1) = 0.843$.  But\n",
        "  note this number depends on your random testing-validation split!\n"
      ],
      "metadata": {
        "id": "gFr_Kufd1xEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in words:\n",
        "    PW = training_set[i].sum() / training_set.shape[0]\n",
        "    P_NW = 1 - PW\n",
        "    P_YW_YS = training_set[(training_set[i]==1) & (training_set['spam']==1)].shape[0] / training_set['spam'].sum()\n",
        "    P_NW_YS = 1 - P_YW_YS\n",
        "    prYSw1 = (P_YW_YS * SE)/PW\n",
        "    prYSw0 = (P_NW_YS * SE)/P_NW\n",
        "    print(i+\":\")\n",
        "    print(\"probability of email being spam if word is present: \"+str(prYSw1))\n",
        "    print(\"probability of email being spam if word is not present: \"+str(prYSw0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taLdZsBs16B5",
        "outputId": "010d786a-9797-4e21-a290-fbad7c9c58a9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "viagra:\n",
            "probability of email being spam if word is present: 1.0\n",
            "probability of email being spam if word is not present: 0.15953307392996108\n",
            "deadline:\n",
            "probability of email being spam if word is present: 0.0\n",
            "probability of email being spam if word is not present: 0.18839103869653767\n",
            "million:\n",
            "probability of email being spam if word is present: 0.8272727272727272\n",
            "probability of email being spam if word is not present: 0.1265880217785844\n",
            "and:\n",
            "probability of email being spam if word is present: 0.15599268069533395\n",
            "probability of email being spam if word is not present: 0.22656249999999986\n",
            "unsubscribe:\n",
            "probability of email being spam if word is present: 0.8947368421052632\n",
            "probability of email being spam if word is not present: 0.15381263616557733\n",
            "urgent:\n",
            "probability of email being spam if word is present: 0.25000000000000006\n",
            "probability of email being spam if word is not present: 0.15942658557775843\n",
            "cialis:\n",
            "probability of email being spam if word is present: 0.05970149253731344\n",
            "probability of email being spam if word is not present: 0.1628838451268358\n",
            "loan:\n",
            "probability of email being spam if word is present: 0.46153846153846156\n",
            "probability of email being spam if word is not present: 0.15472527472527475\n",
            "suspended:\n",
            "probability of email being spam if word is present: 0.7999999999999999\n",
            "probability of email being spam if word is not present: 0.1585101775660459\n",
            "opt-out:\n",
            "probability of email being spam if word is present: 1.0\n",
            "probability of email being spam if word is not present: 0.15844155844155844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. (6pt)  Which of these probabilities have to sum to one? (E.g. $Pr(category = 1) + Pr(category = 0) = 1$.) Which ones do not?  Explain!"
      ],
      "metadata": {
        "id": "Svf5h-ZE6pZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(P_YS_YW + P_NS_YW) # (Pr(S=1) | w = 1 )) + (Pr(S = 0) | w = 1 ))\n",
        "print(P_YS_NW + P_NS_NW) # (Pr(S = 1) | w = 0 )) + (Pr(S = 0) | w = 0 ))\n",
        "print(P_YW_YS + P_NW_YS) # (Pr(w = 1) | S = 1) + (Pr(w = 0) | S = 1)\n",
        "print(P_YW_YN + P_NW_YN) # (Pr(w = 1) | S = 0) + (Pr(w = 0) | S = 0)\n",
        "\n",
        "#The reason behind why these probabilities sum to one is because they have a common denominator (normalizer)"
      ],
      "metadata": {
        "id": "9yowMk1i609G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are done with the estimator.  Your fitted model is completely\n",
        "described by these probabilities.  Let's now turn to prediction, using\n",
        "your validation data.  Note that we are still inside the loop over\n",
        "each word $w$!\n",
        "\n",
        "9. (8pt) For each email in your validation set, predict whether it is predicted to be spam or non-spam.  Hint: you should check if it contains the word $w$ and use the appropriate probability, $Pr(category = S|w = 1)$ or $Pr(category = S|w = 0)$.\n",
        "\n",
        "10. (5pt) Print the resulting confusion matrix and compute accuracy, precision and recall."
      ],
      "metadata": {
        "id": "ugJqV0qRNfPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "valid_set = pd.concat([Xv, yv], axis=1)\n",
        "for i in words:\n",
        "    PW = training_set[i].sum() / training_set.shape[0]\n",
        "    P_NW = 1 - PW\n",
        "    P_YW_YS = training_set[(training_set[i]==1) & (training_set['spam']==1)].shape[0] / training_set['spam'].sum()\n",
        "    P_NW_YS = 1 - P_YW_YS\n",
        "    prYSw1 = (P_YW_YS * SE)/PW\n",
        "    prYSw0 = (P_NW_YS * SE)/P_NW\n",
        "    print(i+\":\")\n",
        "    print(str(prYSw1))\n",
        "    print(str(prYSw0))\n",
        "\n",
        "    P_S_W = np.where(valid_set[i], prYSw1, prYSw0)\n",
        "    predictedSpam = (P_S_W > 0.5) + 0\n",
        "\n",
        "    #cm = confusion_matrix(valid_set['spam'], predictedSpam)\n",
        "    accuracy = accuracy_score(valid_set['spam'], predictedSpam)\n",
        "    recall = recall_score(valid_set['spam'], predictedSpam)\n",
        "    precision = precision_score(valid_set['spam'], predictedSpam)\n",
        "\n",
        "    #print(\"cm: \"+ str(cm))\n",
        "    print(\"precision: \"+ str(precision))\n",
        "    print(\"accuracy: \"+ str(accuracy))\n",
        "    print(\"recall:\" + str(recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsEj7ZZhNg24",
        "outputId": "8d6a9814-7f47-49ae-ff04-1dd3a4f404a9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "viagra:\n",
            "1.3127027027027027\n",
            "0.20941949731833\n",
            "precision: 1.0\n",
            "accuracy: 0.8393782383419689\n",
            "recall:0.010638297872340425\n",
            "deadline:\n",
            "0.0\n",
            "0.24730142566191446\n",
            "precision: 0.0\n",
            "accuracy: 0.8376511226252159\n",
            "recall:0.0\n",
            "million:\n",
            "1.085963144963145\n",
            "0.16617243831853631\n",
            "precision: 0.7037037037037037\n",
            "accuracy: 0.8566493955094991\n",
            "recall:0.20212765957446807\n",
            "and:\n",
            "0.2047720135506046\n",
            "0.29740920608108096\n",
            "precision: 0.0\n",
            "accuracy: 0.8376511226252159\n",
            "recall:0.0\n",
            "unsubscribe:\n",
            "1.1745234708392605\n",
            "0.20191026320438088\n",
            "precision: 1.0\n",
            "accuracy: 0.846286701208981\n",
            "recall:0.05319148936170213\n",
            "urgent:\n",
            "0.32817567567567574\n",
            "0.20927970977058727\n",
            "precision: 0.0\n",
            "accuracy: 0.8376511226252159\n",
            "recall:0.0\n",
            "cialis:\n",
            "0.07837031060911659\n",
            "0.2138180637246058\n",
            "precision: 0.0\n",
            "accuracy: 0.8376511226252159\n",
            "recall:0.0\n",
            "loan:\n",
            "0.605862785862786\n",
            "0.20310828630828634\n",
            "precision: 0.14285714285714285\n",
            "accuracy: 0.8290155440414507\n",
            "recall:0.010638297872340425\n",
            "suspended:\n",
            "1.0501621621621622\n",
            "0.2080767384968338\n",
            "precision: 0.0\n",
            "accuracy: 0.8376511226252159\n",
            "recall:0.0\n",
            "opt-out:\n",
            "1.3127027027027027\n",
            "0.20798666198666202\n",
            "precision: 0.0\n",
            "accuracy: 0.8376511226252159\n",
            "recall:0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. (5pt) Which steps above constitute model training?  In which steps do you use trained model?  What is a trained model in this case? Explain!\n",
        "  \n",
        "  Hint: a trained model is all you need to make predictions."
      ],
      "metadata": {
        "id": "zBEq8sbPQxIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*the steps that consititute as model training would be calculating the naive bayes probabilities. In this assignment model training would constitute from q1 - q6. q9 and 10 would be utilizing the trained model to predict if an email was spam or not spam in the validation set.*"
      ],
      "metadata": {
        "id": "p3aSExkBQ8mX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Now it is time to look at your results a little bit closer.\n",
        "\n",
        "12. (4pt) Comment the overall performance of the model--how do accuracy, precision and recall look like?\n",
        "\n",
        "\n",
        "13. (8pt) Explain why do you see very low recall while the other indicators do not look that bad.\n",
        "\n",
        "\n",
        "14. (8pt) Explain why some words work well and others not:\n",
        "  * why does ''million'' improve accuracy?\n",
        "  * why does ''viagra'' not work?\n",
        "  * why does ''deadline'' not work?\n",
        "  * why does ''and'' not work?\n",
        "\n",
        "  Hint: You may just see where in which emails these words occur, and\n",
        "  how frequently.  These are all different reasons!"
      ],
      "metadata": {
        "id": "7TM_VP_6P6xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer 12. The accuracies for the words are mostly in 80 - 85 range indicating the model overall predicts if an email is spam or not spam pretty well. However the percision and recall are very low. This means the model doesn't do well in terms determining percision meaning there might be a lot of false positives. In addition recall is low indicating there's missing instances that aren't showing up in the results.*"
      ],
      "metadata": {
        "id": "YbAlwcbUUF1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer 13. The low recall may be due higher false negatives. It may be because the outcome is of an imbalanced class or the model hyperparameters are untuned. In order to improve it we can implement class weights.*"
      ],
      "metadata": {
        "id": "EwmUa92pgDo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer 14. Million based on the prior probabilities, shows up in almost half of spam emails. Viagra doesn't work because it only appears in one email. Deadline doesn't show up in spam emails despite showing up in multiple emails. And shows up in many emails however the ratio of it showing up in spam emails is small, therefore not really indicating much.*"
      ],
      "metadata": {
        "id": "VnXCWO1BkEhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"million:\" +str(training_set['million'].sum()) +\" In spam emails: \"+str(training_set[(training_set['million']==1) & (training_set['spam']==1)].shape[0]))\n",
        "print(\"viagra:\" +str(training_set['viagra'].sum()) + \" In spam emails: \"+str(training_set[(training_set['viagra']==1) & (training_set['spam']==1)].shape[0]))\n",
        "print(\"deadline:\" +str(training_set['deadline'].sum()) + \" In spam emails: \"+str(training_set[(training_set['deadline']==1) & (training_set['spam']==1)].shape[0]))\n",
        "print(\"and:\" +str(training_set['and'].sum()) + \" In spam emails: \"+str(training_set[(training_set['and']==1) & (training_set['spam']==1)].shape[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UJ-ZrFvkNKb",
        "outputId": "746182f6-94e1-4080-f528-ae892b5e7e0d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "million:110 In spam emails: 91\n",
            "viagra:1 In spam emails: 1\n",
            "deadline:350 In spam emails: 0\n",
            "and:2186 In spam emails: 341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "15. (5pt) Add such smoothing to the model.  You can either literally add two such lines of data, or alternatively manipulate the way you compute the probabilities.\n",
        "\n",
        "\n",
        "16. (5pt) Repeat the tasks above: compute the probabilities, do predictions, compute the accuracy, precision, recall for all words.  "
      ],
      "metadata": {
        "id": "hwOyVTYZpTUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "valid_set = pd.concat([Xv, yv], axis=1)\n",
        "for i in words:\n",
        "    alpha = 0.05\n",
        "\n",
        "    SE = (training_set['spam'].mean()) + alpha\n",
        "    NSE = (1 - SE) + alpha\n",
        "\n",
        "    PW = training_set[i].sum() / training_set.shape[0]\n",
        "    P_NW = 1 - PW\n",
        "\n",
        "    P_YW_YS = ((training_set[(training_set[i]==1) & (training_set['spam']==1)].shape[0])+alpha) / ((training_set['spam'].sum())*alpha)\n",
        "    P_NW_YS = 1 - P_YW_YS\n",
        "    prYSw1 = (P_YW_YS * SE)/PW\n",
        "    prYSw0 = (P_NW_YS * SE)/P_NW\n",
        "    print(i+\":\")\n",
        "    print(str(prYSw1))\n",
        "    print(str(prYSw0))\n",
        "\n",
        "    P_S_W = np.where(valid_set[i], prYSw1, prYSw0)\n",
        "    predictedSpam = (P_S_W > 0.5) + 0\n",
        "\n",
        "    cm = confusion_matrix(valid_set['spam'], predictedSpam)\n",
        "    accuracy = accuracy_score(valid_set['spam'], predictedSpam)\n",
        "    recall = recall_score(valid_set['spam'], predictedSpam)\n",
        "    precision = precision_score(valid_set['spam'], predictedSpam)\n",
        "\n",
        "    #print(\"cm: \"+ str(cm))\n",
        "    print(\"precision: \"+ str(precision))\n",
        "    print(\"accuracy: \"+ str(accuracy))\n",
        "    print(\"recall:\" + str(recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azz6WlDTukS7",
        "outputId": "5785b1e5-c378-4d00-f90a-dcb3a626824b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "viagra:\n",
            "27.56675675675676\n",
            "0.19806884705717392\n",
            "precision: 1.0\n",
            "accuracy: 0.8393782383419689\n",
            "recall:0.010638297872340425\n",
            "deadline:\n",
            "0.0037505791505791507\n",
            "0.24663304343039577\n",
            "precision: 0.0\n",
            "accuracy: 0.8376511226252159\n",
            "recall:0.0\n",
            "million:\n",
            "21.73119656019656\n",
            "-0.8642157992838575\n",
            "precision: 0.7037037037037037\n",
            "accuracy: 0.8566493955094991\n",
            "recall:0.20212765957446807\n",
            "and:\n",
            "4.096040775450657\n",
            "-66.15816511824323\n",
            "precision: 0.15441176470588236\n",
            "accuracy: 0.18825561312607944\n",
            "recall:0.8936170212765957\n",
            "unsubscribe:\n",
            "23.559559032716926\n",
            "0.016587528705175762\n",
            "precision: 1.0\n",
            "accuracy: 0.846286701208981\n",
            "recall:0.05319148936170213\n",
            "urgent:\n",
            "6.672905405405405\n",
            "0.17620553220466342\n",
            "precision: 0.4\n",
            "accuracy: 0.8359240069084629\n",
            "recall:0.02127659574468085\n",
            "cialis:\n",
            "1.5869987898346107\n",
            "0.16883448201205212\n",
            "precision: 0.047619047619047616\n",
            "accuracy: 0.8048359240069085\n",
            "recall:0.010638297872340425\n",
            "loan:\n",
            "12.150914760914763\n",
            "0.005193109593109588\n",
            "precision: 0.14285714285714285\n",
            "accuracy: 0.8290155440414507\n",
            "recall:0.010638297872340425\n",
            "suspended:\n",
            "21.26578378378378\n",
            "0.16430103121744527\n",
            "precision: 0.0\n",
            "accuracy: 0.8376511226252159\n",
            "recall:0.0\n",
            "opt-out:\n",
            "26.58222972972973\n",
            "0.16422990522990527\n",
            "precision: 0.0\n",
            "accuracy: 0.8376511226252159\n",
            "recall:0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. (4pt) Comment on the results.  Does smoothing improve the overall performance?"
      ],
      "metadata": {
        "id": "Anx2ERG3QyIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Based on results we can see the results for accuracy is pretty similar to the previous model. In addition the percision and accuracy has gone higher ( especially for \"and\" ). However we can also see the naive bayes probabilities is pretty extreme for words like \"and\" and viagra. This suggests there's some overconfidence in the model.*"
      ],
      "metadata": {
        "id": "J_X6SK8XP7mV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXCcYjfvDTTG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}